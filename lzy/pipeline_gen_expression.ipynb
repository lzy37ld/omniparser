{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_all = \"\"\"\n",
    "You are given an input consisting of a screenshot annotated with bounding boxes, each assigned a unique ID and a dict where the key is the ID of the bounding box and the value is the extracted text from that bounding box. Your task consists of two subtasks: First, identify the regions where a user is likely to interact with the content by performing a mouse drag action, based on the screenshot and the dict; and Second, generate a natural language referring expression that specifies a target text span contained within the identified region(s), either entirely within a single bounding box or spanning multiple bounding boxes.\n",
    "\n",
    "For each referring expression, you must classify it into one or more of the following four base categories or their compositions. Each category is defined below, along with illustrative examples.\n",
    "\n",
    "1. Semantic:\n",
    "\n",
    "##Definition##\n",
    "Describes the target text based on its meaning, intent, or topical content.\n",
    "\n",
    "##Examples##\n",
    "a.Select the paragraph discussing how to download models.\n",
    "b.Select the lines that infer the causes of failure.\n",
    "c.Select the sentence about Kobe Bryant’s career.\n",
    "d.Select consecutive words referring to the weight of the MacBook Pro.\n",
    "\n",
    "2. Positional:\n",
    "\n",
    "##Definition##\n",
    "Refers to the location of the text—either in absolute terms (e.g., top, bottom of the page) or relative to other visual elements.\n",
    "\n",
    "##Examples##\n",
    " a. Select the paragraph at the bottom of the page.\n",
    " b. Select the first three lines from the top.\n",
    " c. Select the sentence immediately below the chart title.\n",
    " d. Select the words on the left side of the login button.\n",
    "\n",
    "3. Lexical:\n",
    "\n",
    "##Definition##\n",
    "Describes the text by referencing its literal or quoted content, including the starting words, key phrases, or exact match.\n",
    "\n",
    "##Examples##\n",
    " a. Select the paragraph that begins with “To get started with Python…”.\n",
    " b. Select the lines ending with “before submission is due”.\n",
    " c. Select the sentence containing the phrase “AI is transforming industries”.\n",
    " d. Select the words that say “Monday, Tuesday, and so on”.\n",
    "\n",
    "4. Visual:\n",
    "\n",
    "##Definition##\n",
    "Refers to distinctive visual features of the text, such as font color, size, emphasis, or highlighting.\n",
    "\n",
    "##Examples##\n",
    " a. Select the paragraph written in bold italics.\n",
    " b. Select the lines highlighted in yellow.\n",
    " c. Select the sentence in red font.\n",
    " d. Select the words with the largest font size on the screen.\n",
    "\n",
    "##Note##\n",
    "The bounding box around the text will have color, but they are not the visual attributes of the text. Don’t confuse them!\n",
    "\n",
    "5. Compositional:\n",
    "\n",
    "##Definition##\n",
    "Combines two or more of the base categories to generate a referring expression that draws from multiple dimensions (e.g., semantic and positional).\n",
    "\n",
    "##Examples##\n",
    "\n",
    "a. Select the paragraph at the bottom of the page that discusses how to download the model. (Positional + Semantic)\n",
    "b. Select the sentence in red font starting with “Warning:”. (Visual + Lexical)\n",
    "c. Select the bold text immediately above the login form that mentions error messages. (Visual + Positional + Semantic)\n",
    "d. Select the words on the top-right that say “Subscribe now”. (Positional + Lexical)\n",
    "\n",
    "Every generated expression must be grounded in the bounding boxes provided in the screenshot. That is, the start and end positions of the referenced text should be inferable directly from the involved bounding boxes’ coordinates. For instance, if a bounding box encompasses an entire paragraph, referencing only a sentence within it is not valid unless the bounding box also precisely matches that sentence.\n",
    "\n",
    "The referring expression should be clear about the granularity of the text, i.e., clearly specify if they are pargagraph(s), line(s), sentence(s), word(s) without using ambiguous words like 'text', 'part'.\n",
    "\n",
    "You should output the involved id(s) of the bounding box, the corresponding generated referring expressions, and the category (or categories) it belongs to. Prioritize generating expression that belongs to a single category.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Output(BaseModel):\n",
    "    available: bool\n",
    "    expressions: list[str]\n",
    "\n",
    "system_prompt_semantic = \"\"\"\n",
    "You are given an screenshot input. Your task is to generate natural language referring expressions which specify different target text spans contained within the screenshot that human tend to use mouse drag action to select. Ignore the parts that are not text, that are not selectable by mouse and that are not the places where human tend to use mouse drag action to select in daily life.\n",
    "\n",
    "For the referring expression you generated, they must describe the target text span based on its meaning, intent, or topical content.\n",
    "\n",
    "For example:\n",
    "a.Select the paragraph discussing how to download models.\n",
    "b.Select the lines that infer the causes of failure.\n",
    "c.Select the sentence about Kobe Bryant's career.\n",
    "d.Select consecutive words referring to the weight of the MacBook Pro.\n",
    "\n",
    "The referring expression should be clear about the granularity of the text, i.e., clearly specify if they are pargagraph(s), line(s), sentence(s), words without using ambiguous words like 'text', 'part'. The target text span can be single or multiple paragraphs, lines, sentences. For words, it should be at least multiple words as selecting a single word usually does not require a mouse drag action.\n",
    "\n",
    "If no feasible or available referring expression meeting the requirements can be generated, you should return False and an empty list.\n",
    "If it does, you should return True and the generated referring expressions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt_positional = \"\"\"\n",
    "You are given an screenshot input. Your task is to generate natural language referring expressions which specify different target text spans contained within the screenshot that human tend to use mouse drag action to select. Ignore the parts that are not text, that are not selectable by mouse and that are not the places where human tend to use mouse drag action to select in daily life.\n",
    "\n",
    "For the referring expression you generated, they must refer to the location of the text—either in absolute terms (e.g., top, bottom of the page) or relative to other visual elements.\n",
    "\n",
    "For example:\n",
    " a. Select the paragraph at the bottom of the page.\n",
    " b. Select the first three lines from the top.\n",
    " c. Select the sentence immediately below the chart title.\n",
    " d. Select the words on the left side of the login button.\n",
    "\n",
    "The referring expression should be clear about the granularity of the text, i.e., clearly specify if they are pargagraph(s), line(s), sentence(s), words without using ambiguous words like 'text', 'part'. The target text span can be single or multiple paragraphs, lines, sentences. For words, it should be at least multiple words as selecting a single word usually does not require a mouse drag action.\n",
    "\n",
    "If no feasible or available referring expression meeting the requirements can be generated, you should return False and an empty list.\n",
    "If it does, you should return True and the generated referring expressions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt_lexical = \"\"\"\n",
    "You are given an screenshot input. Your task is to generate natural language referring expressions which specify different target text spans contained within the screenshot that human tend to use mouse drag action to select. Ignore the parts that are not text, that are not selectable by mouse and that are not the places where human tend to use mouse drag action to select in daily life.\n",
    "\n",
    "For the referring expression you generated, they must describe the text by referencing its literal or quoted content, including the starting words, key phrases, or exact match.\n",
    "\n",
    "For example:\n",
    " a. Select the paragraph that begins with “To get started with Python…”.\n",
    " b. Select the lines ending with “before submission is due”.\n",
    " c. Select the sentence containing the phrase “AI is transforming industries”.\n",
    " d. Select the words that say “Monday, Tuesday, and so on”.\n",
    "\n",
    "The referring expression should be clear about the granularity of the text, i.e., clearly specify if they are pargagraph(s), line(s), sentence(s), words without using ambiguous words like 'text', 'part'. The target text span can be single or multiple paragraphs, lines, sentences. For words, it should be at least multiple words as selecting a single word usually does not require a mouse drag action.\n",
    "\n",
    "If no feasible or available referring expression meeting the requirements can be generated, you should return False and an empty list.\n",
    "If it does, you should return True and the generated referring expressions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt_visual = \"\"\"\n",
    "You are given an screenshot input. Your task is to generate natural language referring expressions which specify different target text spans contained within the screenshot that human tend to use mouse drag action to select. Ignore the parts that are not text, that are not selectable by mouse and that are not the places where human tend to use mouse drag action to select in daily life.\n",
    "\n",
    "For the referring expression you generated, they must refer to distinctive visual features of the text, such as font color, size, emphasis, or highlighting.\n",
    "\n",
    "For example:\n",
    " a. Select the paragraph written in bold italics.\n",
    " b. Select the lines highlighted in yellow.\n",
    " c. Select the sentence in red font.\n",
    " d. Select the words with the largest font size on the screen.\n",
    "\n",
    "The referring expression should be clear about the granularity of the text, i.e., clearly specify if they are pargagraph(s), line(s), sentence(s), words without using ambiguous words like 'text', 'part'. The target text span can be single or multiple paragraphs, lines, sentences. For words, it should be at least multiple words as selecting a single word usually does not require a mouse drag action.\n",
    "\n",
    "If no feasible or available referring expression meeting the requirements can be generated, you should return False and an empty list.\n",
    "If it does, you should return True and the generated referring expressions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_map = {\n",
    "    \"semantic\": system_prompt_semantic,\n",
    "    \"positional\": system_prompt_positional,\n",
    "    \"lexical\": system_prompt_lexical,\n",
    "    \"visual\": system_prompt_visual\n",
    "}\n",
    "\n",
    "output_map = {\n",
    "    \"all\": None,\n",
    "    \"semantic\": Output,\n",
    "    \"positional\": Output,\n",
    "    \"lexical\": Output,\n",
    "    \"visual\": Output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completions API\n",
    "\n",
    "def call_llm(model, system_prompt, input_text, image_path,**kwargs):\n",
    "  base64_image = encode_image(image_path)\n",
    "\n",
    "  # chat.completions.create\n",
    "\n",
    "  response = client.beta.chat.completions.parse(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": system_prompt\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": input_text,\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    **kwargs\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def generate_referring_expressions(image_path, model, category, save_dir = \"/home/t-zeyiliao/OmniParser/referring_expressions\"\n",
    "):\n",
    "    input_text = \"Here is the screenshot.\"\n",
    "\n",
    "    res = call_llm(model=model, input_text=input_text, image_path=image_path, system_prompt=system_prompt_map[category], response_format=output_map[category])\n",
    "\n",
    "    expressions = res.choices[0].message.parsed.expressions\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'{image_name}_referring_expressions_model-{model}_category-{category}.json')\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"expressions\":expressions}, f,indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/t-zeyiliao/OmniParser/lzy_images/test.png\"\n",
    "\n",
    "model = \"o4-mini-2025-04-16\"\n",
    "# model = \"gpt-4o-mini\"\n",
    "# model = \"gpt-4o\"\n",
    "\n",
    "category = \"positional\"\n",
    "\n",
    "generate_referring_expressions(image_path, model, category)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
