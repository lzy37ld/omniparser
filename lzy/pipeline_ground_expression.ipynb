{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Output_one(BaseModel):\n",
    "    is_grounded: bool\n",
    "    ids: list[str]\n",
    "\n",
    "class Output_three(BaseModel):\n",
    "    level: str # paragraph, line, word\n",
    "    is_grounded: bool\n",
    "    ids: list[str]\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are given an input consisting of 1. a screenshot annotated with bounding boxes, each assigned a unique ID, 2. a dict where the key is the ID of the bounding box and the value is the extracted text from that bounding box and 3. a referring expression that specifies a target text span.\n",
    "\n",
    "Your task is to first identify the target text span given the screenshot and the referring expression. Then, check if the referring expression is grounded in the bounding boxes provided in the screenshot. That is, the coordinates of the start and end positions of the target text span should be inferable directly from the available bounding boxes' coordinates. For instance, if a bounding box encompasses an entire paragraph containing multiple sentences, referencing only a single sentence within it is not valid as you cannot infer the start and end positions of the sentence from the coordinates of the bounding box. Besides, the bounding box might be not accurate or suitbale enough (e.g. the id of bounding box is hidden by other bounding boxes or the id is not visible) for grounding the referring expression. For example, the bounding box is too large or too small for the target text span. If so, you should treat the referring expression as not grounded.\n",
    "\n",
    "If the referring expression is grounded, you should output True and the involved id(s) of the bounding box that can be used to infer the start and end positions of the target text span. If the referring expression is not grounded, you should output False and leave the ids empty.\n",
    "\"\"\"\n",
    "\n",
    "用3个模型。。。\n",
    "逻辑语法，都需要gpt refine\n",
    "\n",
    "class Output_three(BaseModel):\n",
    "    level: str # paragraph, line, word\n",
    "    is_grounded: bool\n",
    "    ids: list[str]\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are given an input consisting of 1. three screenshots annotated with bounding boxes, each assigned a unique ID. They are from the same image but with different granularities of bounding boxes. The first one is marked in the paragraph level, the second one is marked in the line level, and the third one is marked in the word level. 2. three dicts where the key is the ID of the bounding box and the value is the extracted text from that bounding box. They are based on the three screenshots with different levels of bounding box granularities. 3. a referring expression that specifies a target text span.\n",
    "\n",
    "Your task is to first identify the target text span given the screenshots and the referring expression. After that, you have to decide which level of bounding box granularity is the most suitable for grounding the referring expression given three screenshots and dicts.\n",
    "You should then decide if the referring expression can be grounded by the bounding boxes of the most suitable level of granularity; that is, the coordinates of the start and end positions of the target text span should be inferable directly from the available bounding boxes' coordinates. For instance, if a bounding box encompasses an entire paragraph containing multiple sentences, referencing only a single sentence within it is not valid as you cannot infer the start and end positions of the sentence from the coordinates of the bounding box. Therefore, you should first identify the most suitable level of bounding box granularity for grounding the referring expression. If the start and end coordinates of the target text span can be inferred from different screenshots with different levels of granualrity, you should select the one with most tight bounding box. If the bounding box is not inferable directly from the available bounding boxes' coordinates, you should output False for the is_grounded field. However, if all of them are not suitable, you should output False for the is_grounded field. Besides, the bounding box might be not accurate or suitbale at all (e.g. the id of bounding box is hidden by other bounding boxes or the id is not visible) for grounding the referring expression. For example, the bounding box is too large or too small for the target text span. If so, you should treat the referring expression as not grounded.\n",
    "\n",
    "If the referring expression can be grounded, you should output True for the is_grounded field, the level of the bounding box granularity you used for grounding and the involved id(s) of the bounding box that can be used to infer the start and end positions of the target text span (Note that the target text span can span across multiple bounding boxes). If the referring expression is not grounded, you should output False for the is_grounded field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completions API\n",
    "\n",
    "def call_llm(model, system_prompt, input_text, image_path,**kwargs):\n",
    "  base64_image = encode_image(image_path)\n",
    "  # chat.completions.create\n",
    "\n",
    "  response = client.beta.chat.completions.parse(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": system_prompt\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": input_text,\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    **kwargs\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def ground_expression(image_name, paresed_mode, referring_expression_category, referring_expression_model, to_ground_model):\n",
    "        \n",
    "\n",
    "    image_with_bbox_path = f\"/home/t-zeyiliao/OmniParser/parsed_text_images/name-{image_name}_mode-{paresed_mode}.png\"\n",
    "    input_dict_path = f\"/home/t-zeyiliao/OmniParser/parsed_text_text/name-{image_name}_mode-{paresed_mode}.json\"\n",
    "\n",
    "    referring_expression_path = f\"/home/t-zeyiliao/OmniParser/referring_expressions/referring-expressions_name-{image_name}_model-{referring_expression_model}_category-{referring_expression_category}.json\"\n",
    "    grounded_expression_path = referring_expression_path.replace('/referring_expressions/', '/referring_expressions_grounded/').replace('.json', f'_parsed-mode-{paresed_mode}_grounded-by-{to_ground_model}.json')\n",
    "    os.makedirs(os.path.dirname(grounded_expression_path), exist_ok=True)\n",
    "\n",
    "\n",
    "    with open(input_dict_path, 'r') as f:\n",
    "        input_dict = json.load(f)\n",
    "\n",
    "    input_text = 'The dict where the key is the id of the bounding box and the value is the text of the bounding box.' + '/n/n' + json.dumps(input_dict)\n",
    "\n",
    "    with open(referring_expression_path, 'r') as f:\n",
    "        referring_expressions = json.load(f)\n",
    "\n",
    "\n",
    "    res_save_dict = {}\n",
    "    for idx, referring_expression in enumerate(referring_expressions['expressions']['expressions']):\n",
    "        input_text = 'The dict where the key is the id of the bounding box and the value is the text of the bounding box.' + '/n/n' + json.dumps(input_dict需要改一下) + '/n/n' + 'The referring expression is ' + referring_expression\n",
    "\n",
    "        res = call_llm(model=to_ground_model, input_text=input_text, image_path=image_with_bbox_path, system_prompt=system_prompt, response_format=Output)\n",
    "\n",
    "        res_save_dict[idx] = dict(res.choices[0].message.parsed)\n",
    "\n",
    "    with open(f'{grounded_expression_path}', 'w') as f:\n",
    "        json.dump(res_save_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_name = \"test\"\n",
    "paresed_mode = \"paragraph\"\n",
    "\n",
    "referring_expression_category = \"positional\"\n",
    "referring_expression_model = \"gpt-4o\"\n",
    "\n",
    "to_ground_model = \"o4-mini-2025-04-16\"\n",
    "to_ground_model = \"gpt-4o-mini\"\n",
    "to_ground_model = \"gpt-4o\"\n",
    "\n",
    "ground_expression(image_name, paresed_mode, referring_expression_category, referring_expression_model, to_ground_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
