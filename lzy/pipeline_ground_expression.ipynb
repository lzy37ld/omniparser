{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Output(BaseModel):\n",
    "    is_grounded: bool\n",
    "    ids: list[str]\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are given an input consisting of 1. a screenshot annotated with bounding boxes, each assigned a unique ID, 2. a dict where the key is the ID of the bounding box and the value is the extracted text from that bounding box and 3. a referring expression that specifies a target text span.\n",
    "\n",
    "Your task is to first identify the target text span given the screenshot and the referring expression. Then, check if the referring expression is grounded in the bounding boxes provided in the screenshot. That is, the coordinates of the start and end positions of the target text span should be inferable directly from the available bounding boxes' coordinates. For instance, if a bounding box encompasses an entire paragraph containing multiple sentences, referencing only a single sentence within it is not valid as you cannot infer the start and end positions of the sentence from the coordinates of the bounding box.\n",
    "\n",
    "If the referring expression is grounded, you should output True and the involved id(s) of the bounding box that can be used to infer the start and end positions of the target text span. If the referring expression is not grounded, you should output False and leave the ids empty.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completions API\n",
    "\n",
    "def call_llm(model, system_prompt, input_text, image_path,**kwargs):\n",
    "  base64_image = encode_image(image_path)\n",
    "  # chat.completions.create\n",
    "\n",
    "  response = client.beta.chat.completions.parse(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": system_prompt\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": input_text,\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    **kwargs\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "image_with_bbox_path = \"/home/t-zeyiliao/OmniParser/parsed_text_images/test_mode-paragraph.png\"\n",
    "input_dict_path = \"/home/t-zeyiliao/OmniParser/parsed_text_text/test_mode-paragraph.json\"\n",
    "referring_expression_path = \"/home/t-zeyiliao/OmniParser/referring_expressions/test_referring_expressions_model-o4-mini-2025-04-16_category-positional.json\"\n",
    "\n",
    "model = \"o4-mini-2025-04-16\"\n",
    "model = \"gpt-4o-mini\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "with open(input_dict_path, 'r') as f:\n",
    "    input_dict = json.load(f)\n",
    "\n",
    "input_text = 'The dict where the key is the id of the bounding box and the value is the text of the bounding box.' + '/n/n' + json.dumps(input_dict)\n",
    "\n",
    "with open(referring_expression_path, 'r') as f:\n",
    "    referring_expressions = json.load(f)\n",
    "\n",
    "\n",
    "res_save_dict = {}\n",
    "for idx, referring_expression in enumerate(referring_expressions['expressions']):\n",
    "    input_text = 'The dict where the key is the id of the bounding box and the value is the text of the bounding box.' + '/n/n' + json.dumps(input_dict) + '/n/n' + 'The referring expression is ' + referring_expression\n",
    "\n",
    "    res = call_llm(model=model, input_text=input_text, image_path=image_with_bbox_path, system_prompt=system_prompt, response_format=Output)\n",
    "\n",
    "    res_save_dict[idx] = dict(res.choices[0].message.parsed)\n",
    "\n",
    "with open(f'{referring_expression_path.replace(\".json\", \"_grounded.json\")}', 'w') as f:\n",
    "    json.dump(res_save_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_grounded': True, 'ids': ['5']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(res.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_6859f0853b9c81a094b16ff4a923c99f0a730c731d4721a3', created_at=1750724741.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_6859f086678881a08cf338c2c7fde9d30a730c731d4721a3', content=[ResponseOutputText(annotations=[], text='## Involved Id(s): 6\\nReferring Expression: Select the text beginning with \"Note 1: [\\'ch_Sim\\', \\'en\\'] is the list of languages...\"\\nCategory: Lexical\\n\\n## Involved Id(s): 13\\nReferring Expression: Select the text about downloading model weights for the chosen language.\\nCategory: Semantic\\n\\n## Involved Id(s): 14\\nReferring Expression: Select the text about running the model in CPU-only mode if you don\\'t have a GPU.\\nCategory: Semantic', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=2048, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=2593, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=106, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=2699), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
